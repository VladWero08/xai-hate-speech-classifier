{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJoUAqalXKO0"
   },
   "source": [
    "# **Explainer Feature Extraction**\n",
    "\n",
    "In this notebook, the explanation features will be extracted from the attention heads of the model, and the LIME and SHAP scores. Afterwards, they will be saved to be reused for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tko4crmMXI1I",
    "outputId": "bb354dbc-57e9-4556-b5f9-f0a5c4a49933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /venv/main/lib/python3.12/site-packages (2.10.0+cu130)\n",
      "Requirement already satisfied: transformers in /venv/main/lib/python3.12/site-packages (5.1.0)\n",
      "Requirement already satisfied: lime in /venv/main/lib/python3.12/site-packages (0.2.0.1)\n",
      "Requirement already satisfied: shap in /venv/main/lib/python3.12/site-packages (0.50.0)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: datasets in /venv/main/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: nltk in /venv/main/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: matplotlib in /venv/main/lib/python3.12/site-packages (3.10.8)\n",
      "Requirement already satisfied: scipy in /venv/main/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /venv/main/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /venv/main/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: cuda-bindings==13.0.3 in /venv/main/lib/python3.12/site-packages (from torch) (13.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.88 in /venv/main/lib/python3.12/site-packages (from torch) (13.0.88)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.96 in /venv/main/lib/python3.12/site-packages (from torch) (13.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.85 in /venv/main/lib/python3.12/site-packages (from torch) (13.0.85)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.15.1.9 in /venv/main/lib/python3.12/site-packages (from torch) (9.15.1.9)\n",
      "Requirement already satisfied: nvidia-cublas==13.1.0.3 in /venv/main/lib/python3.12/site-packages (from torch) (13.1.0.3)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.61 in /venv/main/lib/python3.12/site-packages (from torch) (12.0.0.61)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /venv/main/lib/python3.12/site-packages (from torch) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.4.66 in /venv/main/lib/python3.12/site-packages (from torch) (12.0.4.66)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.3.3 in /venv/main/lib/python3.12/site-packages (from torch) (12.6.3.3)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /venv/main/lib/python3.12/site-packages (from torch) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.28.9 in /venv/main/lib/python3.12/site-packages (from torch) (2.28.9)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.4.5 in /venv/main/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.85 in /venv/main/lib/python3.12/site-packages (from torch) (13.0.85)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.88 in /venv/main/lib/python3.12/site-packages (from torch) (13.0.88)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.1.6 in /venv/main/lib/python3.12/site-packages (from torch) (1.15.1.6)\n",
      "Requirement already satisfied: triton==3.6.0 in /venv/main/lib/python3.12/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /venv/main/lib/python3.12/site-packages (from cuda-bindings==13.0.3->torch) (1.3.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /venv/main/lib/python3.12/site-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /venv/main/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /venv/main/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /venv/main/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: anyio in /venv/main/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.0)\n",
      "Requirement already satisfied: certifi in /venv/main/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /venv/main/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /venv/main/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /venv/main/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /venv/main/lib/python3.12/site-packages (from lime) (1.8.0)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /venv/main/lib/python3.12/site-packages (from lime) (0.26.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in /venv/main/lib/python3.12/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /venv/main/lib/python3.12/site-packages (from shap) (0.63.1)\n",
      "Requirement already satisfied: cloudpickle in /venv/main/lib/python3.12/site-packages (from shap) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /venv/main/lib/python3.12/site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /venv/main/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: click in /venv/main/lib/python3.12/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /venv/main/lib/python3.12/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /venv/main/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /venv/main/lib/python3.12/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /venv/main/lib/python3.12/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /venv/main/lib/python3.12/site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /venv/main/lib/python3.12/site-packages (from numba>=0.54->shap) (0.46.0)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /venv/main/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (2.37.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /venv/main/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (2026.1.28)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /venv/main/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /venv/main/lib/python3.12/site-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers lime shap pandas datasets nltk matplotlib scipy requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUc9IZ4mXqO3",
    "outputId": "82821808-ede5-4cfd-f1b8-44c1118f296a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import shap\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import scipy as sp\n",
    "import transformers\n",
    "\n",
    "from io import StringIO\n",
    "from datasets import load_dataset\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZeYb4ASXriV"
   },
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951,
     "referenced_widgets": [
      "4b66b10e32e6418ebd6609fddb7f89d1",
      "f6fe071170224a0e95eb69886eac81cf",
      "07448fc0518c40108e238f5bb5e9ba06",
      "1b45d312a84549c2b0087c8c2a9d0fef",
      "388d35a6987f47b9bdf8189b20f05ed1",
      "fc677ea51c894e37b920e61580dbafc9",
      "d92d41f6e1194b8f8eca122e089aab4f",
      "7a96673577ae4ed6b1fc5fd813677ff6",
      "424e0c8b7b464f18af0ff73a05bcbfd9",
      "ed39a1188b39478fb53f9629dd8b336f",
      "8e166f1cc89d46688bd996bab2abd851"
     ]
    },
    "id": "3DaRN5-8XsmC",
    "outputId": "21149482-9b3d-46c1-8acb-cfd3e08407cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5c18381d1141a495336f12c6bc2dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: facebook/roberta-hate-speech-dynabench-r4-target\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}!\")\n",
    "\n",
    "model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "NON_HATE_SPEECH_CLASS = 0\n",
    "HATE_SPEECH_CLASS = 1\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_attentions=True)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9n1U3GcX0um",
    "outputId": "e63b025b-5c9d-4ba2-a58e-f5c452cd147c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "Number of attention heads: 12\n",
      "Number of hidden layers: 12\n"
     ]
    }
   ],
   "source": [
    "num_attention_heads = model.config.num_attention_heads\n",
    "num_hidden_layers = model.config.num_hidden_layers\n",
    "\n",
    "print(f\"Special tokens: {tokenizer.all_special_tokens}\")\n",
    "print(f\"Number of attention heads: {num_attention_heads}\")\n",
    "print(f\"Number of hidden layers: {num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dm-uGUt1YEYZ",
    "outputId": "769a9406-2b43-4bcd-e717-efb0bec0b13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'i want to cut your throat, dont then to kill you!!!!'\n",
      "Tokens: ['<s>', 'i', 'Ġwant', 'Ġto', 'Ġcut', 'Ġyour', 'Ġthroat', ',', 'Ġdont', 'Ġthen', 'Ġto', 'Ġkill', 'Ġyou', '!!!!', '</s>']\n",
      "Predicted label: hate\n",
      "Confidence: 0.9969\n"
     ]
    }
   ],
   "source": [
    "text = \"i want to cut your throat, dont then to kill you!!!!\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "outputs = model(**inputs)\n",
    "probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "predicted_class_id = probabilities.argmax().item()\n",
    "predicted_label = model.config.id2label[predicted_class_id]\n",
    "predicted_score = probabilities[0][predicted_class_id].item()\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "print(f\"Confidence: {predicted_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "uYKtnc9rYPZH"
   },
   "outputs": [],
   "source": [
    "def is_word_token(token: str) -> bool:\n",
    "  global tokenizer\n",
    "\n",
    "  if token in tokenizer.all_special_tokens:\n",
    "    return False\n",
    "\n",
    "  token = token.lstrip(\"Ġ\")\n",
    "  return any(c.isalpha() for c in token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvM5gB9UYSCr",
    "outputId": "1e10a624-6d6f-4339-8c23-fa5e80ec3b5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00311355, 0.99688643]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_probabilities(text: str) -> torch.Tensor:\n",
    "  inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "  probabilities = F.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "  return probabilities\n",
    "\n",
    "# [0]: non hate speech\n",
    "# [1]: hate speech\n",
    "get_probabilities(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW_v_d6kZL5O"
   },
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20HChxuWZM7M",
    "outputId": "e8b90e69-f601-43f1-cb32-cdc0123d0717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC Berkeley Hate Speech Dataset\n",
      "Shape: (135556, 143)\n",
      "Columns: ['comment_id', 'annotator_id', 'platform', 'sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize', 'violence', 'genocide', 'attack_defend', 'hatespeech', 'hate_speech_score', 'text', 'infitms', 'outfitms', 'annotator_severity', 'std_err', 'annotator_infitms', 'annotator_outfitms', 'hypothesis', 'target_race_asian', 'target_race_black', 'target_race_latinx', 'target_race_middle_eastern', 'target_race_native_american', 'target_race_pacific_islander', 'target_race_white', 'target_race_other', 'target_race', 'target_religion_atheist', 'target_religion_buddhist', 'target_religion_christian', 'target_religion_hindu', 'target_religion_jewish', 'target_religion_mormon', 'target_religion_muslim', 'target_religion_other', 'target_religion', 'target_origin_immigrant', 'target_origin_migrant_worker', 'target_origin_specific_country', 'target_origin_undocumented', 'target_origin_other', 'target_origin', 'target_gender_men', 'target_gender_non_binary', 'target_gender_transgender_men', 'target_gender_transgender_unspecified', 'target_gender_transgender_women', 'target_gender_women', 'target_gender_other', 'target_gender', 'target_sexuality_bisexual', 'target_sexuality_gay', 'target_sexuality_lesbian', 'target_sexuality_straight', 'target_sexuality_other', 'target_sexuality', 'target_age_children', 'target_age_teenagers', 'target_age_young_adults', 'target_age_middle_aged', 'target_age_seniors', 'target_age_other', 'target_age', 'target_disability_physical', 'target_disability_cognitive', 'target_disability_neurological', 'target_disability_visually_impaired', 'target_disability_hearing_impaired', 'target_disability_unspecific', 'target_disability_other', 'target_disability', 'target_politics_alt_right', 'target_politics_communist', 'target_politics_conservative', 'target_politics_democrat', 'target_politics_green_party', 'target_politics_leftist', 'target_politics_liberal', 'target_politics_libertarian', 'target_politics_republican', 'target_politics_socialist', 'target_politics_other', 'target_politics', 'annotator_gender', 'annotator_trans', 'annotator_educ', 'annotator_income', 'annotator_ideology', 'annotator_gender_men', 'annotator_gender_women', 'annotator_gender_non_binary', 'annotator_gender_prefer_not_to_say', 'annotator_gender_self_describe', 'annotator_transgender', 'annotator_cisgender', 'annotator_transgender_prefer_not_to_say', 'annotator_education_some_high_school', 'annotator_education_high_school_grad', 'annotator_education_some_college', 'annotator_education_college_grad_aa', 'annotator_education_college_grad_ba', 'annotator_education_professional_degree', 'annotator_education_masters', 'annotator_education_phd', 'annotator_income_<10k', 'annotator_income_10k-50k', 'annotator_income_50k-100k', 'annotator_income_100k-200k', 'annotator_income_>200k', 'annotator_ideology_extremeley_conservative', 'annotator_ideology_conservative', 'annotator_ideology_slightly_conservative', 'annotator_ideology_neutral', 'annotator_ideology_slightly_liberal', 'annotator_ideology_liberal', 'annotator_ideology_extremeley_liberal', 'annotator_ideology_no_opinion', 'annotator_race_asian', 'annotator_race_black', 'annotator_race_latinx', 'annotator_race_middle_eastern', 'annotator_race_native_american', 'annotator_race_pacific_islander', 'annotator_race_white', 'annotator_race_other', 'annotator_age', 'annotator_religion_atheist', 'annotator_religion_buddhist', 'annotator_religion_christian', 'annotator_religion_hindu', 'annotator_religion_jewish', 'annotator_religion_mormon', 'annotator_religion_muslim', 'annotator_religion_nothing', 'annotator_religion_other', 'annotator_sexuality_bisexual', 'annotator_sexuality_gay', 'annotator_sexuality_straight', 'annotator_sexuality_other']\n"
     ]
    }
   ],
   "source": [
    "ucb_hate_speech_ds = load_dataset(\"ucberkeley-dlab/measuring-hate-speech\")\n",
    "ucb_hate_speech_df = ucb_hate_speech_ds[\"train\"].to_pandas()\n",
    "\n",
    "print(\"UC Berkeley Hate Speech Dataset\")\n",
    "print(\"Shape:\", ucb_hate_speech_df.shape)\n",
    "print(\"Columns:\", ucb_hate_speech_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "HE3oQsrNZceO"
   },
   "outputs": [],
   "source": [
    "ucb_categories = [\"race\", \"religion\", \"origin\", \"gender\", \"sexuality\", \"age\", \"disability\", \"politics\"]\n",
    "ucb_hate_speech_df.rename(columns={\n",
    "    \"target_race\": \"race\",\n",
    "    \"target_religion\": \"religion\",\n",
    "    \"target_origin\": \"origin\",\n",
    "    \"target_gender\": \"gender\",\n",
    "    \"target_sexuality\": \"sexuality\",\n",
    "    \"target_age\": \"age\",\n",
    "    \"target_disability\": \"disability\",\n",
    "    \"target_politics\": \"politics\",\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt0mYQLgZmCi"
   },
   "source": [
    "Next, we need to aggregate the columns by the scores given by each annotator. The hyphotesis score will be the *mean* of all the scores for that comment, and the final hate speech category will be the most voted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "joHqZ8ngZm4q",
    "outputId": "6ab20b8a-be40-4d87-dc0a-e1ab651b225c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC Berkeley Hate Speech Dataset Aggregated\n",
      "Shape: (39565, 144)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1966/2445370064.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ucb_hate_speech_df = ucb_hate_speech_df.groupby('comment_id', as_index=False).agg(ucb_aggregations)\n",
      "/tmp/ipykernel_1966/2445370064.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ucb_hate_speech_df['hate_speech_label'] = ucb_hate_speech_df[ucb_categories].idxmax(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# gather the aggregate type for each comment\n",
    "ucb_aggregations = {}\n",
    "for col in ucb_hate_speech_df.columns:\n",
    "    if col == 'comment_id':\n",
    "        continue\n",
    "\n",
    "    if col == 'hypothesis':\n",
    "        ucb_aggregations[col] = 'mean'\n",
    "    elif ucb_hate_speech_df[col].dtype == 'bool':\n",
    "        ucb_aggregations[col] = 'sum'\n",
    "    else:\n",
    "        ucb_aggregations[col] = 'first'\n",
    "\n",
    "# group each comment based on the annotator's score\n",
    "ucb_hate_speech_df = ucb_hate_speech_df.groupby('comment_id', as_index=False).agg(ucb_aggregations)\n",
    "\n",
    "# add a new column with the hate speech label\n",
    "ucb_hate_speech_df['hate_speech_label'] = ucb_hate_speech_df[ucb_categories].idxmax(axis=1)\n",
    "\n",
    "print(\"UC Berkeley Hate Speech Dataset Aggregated\")\n",
    "print(\"Shape:\", ucb_hate_speech_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-NCqcImZv8V"
   },
   "source": [
    "Extract the comments that are definetely hate speech, depending on their hate speech score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RScW_zgPZyaf",
    "outputId": "f855ce5b-fdd6-48b6-8726-427a82ab4191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only Hate Speech Dataset\n",
      "Shape: (501, 144)\n"
     ]
    }
   ],
   "source": [
    "# keep only the comments with a hate speech score larger than 0.5\n",
    "ucb_hate_speech_df = ucb_hate_speech_df[ucb_hate_speech_df['hypothesis'] >= 0.5]\n",
    "ucb_hate_speech_df = ucb_hate_speech_df.drop_duplicates(subset=['comment_id'])\n",
    "\n",
    "# remove the categories with very few samples\n",
    "ucb_hate_speech_df = ucb_hate_speech_df[~ucb_hate_speech_df[\"hate_speech_label\"].isin([\"politics\", \"disability\"])]\n",
    "\n",
    "print(\"Only Hate Speech Dataset\")\n",
    "print(\"Shape:\", ucb_hate_speech_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaXPzlE2goud",
    "outputId": "98a53fdf-d45b-4d0d-8718-bf9d8e4cf1e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eat cock and die',\n",
       " 'I fucking HATE americans. I hope your country BURNS.',\n",
       " 'kill yourself you fucking idiot cunt',\n",
       " 'Fuck those Saudi rats',\n",
       " 'Fuck off you nazi cunt. Die in a fire.',\n",
       " 'Stupid fucking cunt. I hope your mother dies in crippling agony.',\n",
       " 'Nuke the border area, Nuke Algeria, Nuke Morocco, Nuke the UN',\n",
       " 'Burn their fucking church down!',\n",
       " 'Shoot yourself in the pussy!',\n",
       " 'You are just Another retarded white boy ready to shoot up a school smh you are such trash.... please go to your closet and make a noose and kill yourself your fuckin trash']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ucb_hate_speech_df[:10][\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogFAEWUSaPin"
   },
   "source": [
    "## **Explainers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fui2yjKmaWdc"
   },
   "source": [
    "### **Attention Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZjNThrXJaQqn",
    "outputId": "7ca7072e-2f3b-4d23-f92f-61857f4eb7de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 0.19648334383964539),\n",
       " ('fucking', 0.04118870850652456),\n",
       " ('H', 0.18134414218366146),\n",
       " ('ATE', 0.33032047376036644),\n",
       " ('americ', 0.8735839948058128),\n",
       " ('ans', 0.5834043361246586),\n",
       " ('.', 0.5437215864658356),\n",
       " ('I', 0.181207574903965),\n",
       " ('hope', 0.06883443333208561),\n",
       " ('your', 0.19421817548573017),\n",
       " ('country', 0.10570773109793663),\n",
       " ('BUR', 0.0880456380546093),\n",
       " ('NS', 0.07501181587576866),\n",
       " ('.', 0.5369281955063343)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_token_attention_score(\n",
    "  attentions: tuple[torch.Tensor],\n",
    "  token_pos: int,\n",
    "  last_n_layers: int = 4\n",
    ") -> float:\n",
    "  \"\"\"\n",
    "  Given the attention matrix resulted from BERT inference, and a token position,\n",
    "  computes the attention score for that token by summing the attention of the\n",
    "  last 4 layers of BERT.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  attentions : tuple[torch.Tensor]\n",
    "             Attention resulted from BERT inference of a tokenized paragraph.\n",
    "  token_pos  : int\n",
    "             Position of the token, should be > 0 (class token)\n",
    "  \"\"\"\n",
    "  # (num_layers, num_heads, query, key)\n",
    "  # the query in this case will be token 0, corresponding to the class token\n",
    "  # the key in this case will be the token_pos received as parameter\n",
    "  attention_score = 0\n",
    "  for i in range(len(attentions) - 1, len(attentions) - last_n_layers - 1, -1):\n",
    "      attention_score += attentions[i][:, :, 0, token_pos].mean().item()\n",
    "\n",
    "  return attention_score\n",
    "\n",
    "def compute_attention_scores(\n",
    "    attentions: tuple[torch.Tensor],\n",
    "    tokens: list[str],\n",
    "    last_n_layers: int = 4\n",
    ") -> list[tuple]:\n",
    "  \"\"\"\n",
    "  Given the attention matrix resulted from BERT inference, computes the attention\n",
    "  score for each token by summing the attention of the last 4 layers of BERT.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  attentions : tuple[torch.Tensor]\n",
    "              Attention resulted from BERT inference of a tokenized paragraph.\n",
    "  \"\"\"\n",
    "  global tokenizer\n",
    "  attention_scores = []\n",
    "\n",
    "  for i in range(len(tokens)):\n",
    "    token = tokens[i].lstrip(\"Ġ\")\n",
    "    attention_scores.append(\n",
    "        (token, compute_token_attention_score(attentions, i, last_n_layers))\n",
    "    )\n",
    "\n",
    "  return attention_scores\n",
    "\n",
    "inputs = tokenizer(ucb_hate_speech_df[:4][\"text\"].tolist()[1], return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "outputs = model(**inputs)\n",
    "\n",
    "compute_attention_scores(outputs.attentions, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Nt3Q37SaaY7"
   },
   "source": [
    "### **LIME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjjpyWouaUb2",
    "outputId": "7600c171-02df-4d6f-f13f-c3778c6480b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', -0.3403289444333246),\n",
       " ('fucking', 0.14628814036484936),\n",
       " ('BURNS', -0.13557863173949833),\n",
       " ('your', 0.10478344049653388),\n",
       " ('country', -0.05991141416140099),\n",
       " ('americans', 0.029500551878973054),\n",
       " ('hope', 0.014137257887937729),\n",
       " ('HATE', 0.0025525026786472546)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_lime_scores(\n",
    "  text: str,\n",
    "  tokens: list[str],\n",
    "  label: int = HATE_SPEECH_CLASS,\n",
    "  num_samples: int = 100,\n",
    ") -> dict:\n",
    "  \"\"\"\n",
    "  Given a text, its tokens and the predicted class by the hate speech classifier, it\n",
    "  uses LIME to extract the importance of each token. The number of features used by\n",
    "  the LimeTextExplainer is equal to the total number of non special tokens.\n",
    "  \"\"\"\n",
    "  global model, tokenizer\n",
    "\n",
    "  # compute the number of features as the number of tokens different from the special ones\n",
    "  num_features = len([token for token in tokens if token not in tokenizer.all_special_tokens])\n",
    "\n",
    "  # define the LIME explainer\n",
    "  explainer = LimeTextExplainer(class_names=list(model.config.id2label.values()))\n",
    "  explanation = explainer.explain_instance(\n",
    "      text,\n",
    "      get_probabilities,\n",
    "      num_features=num_features,\n",
    "      num_samples=num_samples,\n",
    "  )\n",
    "\n",
    "  # extract the LIME scores and store them in a dictionary\n",
    "  feature_attributions = explanation.as_list(label=label)\n",
    "  tokens = [(word.item(), score) for _, (word, score) in enumerate(feature_attributions)]\n",
    "\n",
    "  return tokens\n",
    "\n",
    "# compute the LIME score for the tokens in the example sentence\n",
    "compute_lime_scores(ucb_hate_speech_df[:4][\"text\"].tolist()[1], tokens, predicted_class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_7eNTmgbQuC"
   },
   "source": [
    "### **SHAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "9I8ffJLlvgvo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I ', 0.9005785882472992),\n",
       " ('fucking ', 0.027191132307052612),\n",
       " ('H', 0.11832618117332458),\n",
       " ('ATE ', 0.29801749587059023),\n",
       " ('americ', 0.1651911675930023),\n",
       " ('ans', 0.1309647500514984),\n",
       " ('. ', 0.415344113111496),\n",
       " ('I ', 0.174204843384879),\n",
       " ('hope ', 0.42708660875047955),\n",
       " ('your ', 0.27539290700639996),\n",
       " ('country ', 0.16818346296037948),\n",
       " ('BUR', 0.09716351543154034),\n",
       " ('NS', 0.003816204411642876),\n",
       " ('.', 0.01946146999086651)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a prediction function\n",
    "def f(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding=\"max_length\", max_length=500, truncation=True, add_special_tokens=False) for v in x]).to(device)\n",
    "    with torch.no_grad():\n",
    "      outputs = model(tv)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units\n",
    "    return val\n",
    "\n",
    "# build an explainer using a token masker\n",
    "shap_explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "def compute_shap_scores(text: str) -> np.ndarray:\n",
    "  global shap_explainer\n",
    "  shap_values = shap_explainer([text], fixed_context=1, batch_size=1)\n",
    "  shap_values_list = []\n",
    "\n",
    "  for i, value in enumerate(shap_values.values[0]):\n",
    "      token = shap_values.data[0][i]\n",
    "      if token == '' or token in tokenizer.all_special_tokens:\n",
    "          continue\n",
    "          \n",
    "      shap_values_list.append((token, value.item()))\n",
    "\n",
    "  return shap_values_list\n",
    "\n",
    "compute_shap_scores(ucb_hate_speech_df[:4][\"text\"].tolist()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extend Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1966/2787616032.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ucb_hate_speech_df[['attention_scores', 'lime_scores', 'shap_scores']] = ucb_hate_speech_df.apply(compute_all_explanations, axis=1)\n",
      "/tmp/ipykernel_1966/2787616032.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ucb_hate_speech_df[['attention_scores', 'lime_scores', 'shap_scores']] = ucb_hate_speech_df.apply(compute_all_explanations, axis=1)\n",
      "/tmp/ipykernel_1966/2787616032.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ucb_hate_speech_df[['attention_scores', 'lime_scores', 'shap_scores']] = ucb_hate_speech_df.apply(compute_all_explanations, axis=1)\n"
     ]
    }
   ],
   "source": [
    "def compute_all_explanations(row):\n",
    "    global tokenizer\n",
    "    text = row['text']\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # compute attention scores\n",
    "    attention_scores = compute_attention_scores(outputs.attentions, tokens)\n",
    "    # compute LIME scores\n",
    "    lime_scores = compute_lime_scores(text, tokens)\n",
    "    # compute SHAP scores\n",
    "    shap_scores = compute_shap_scores(text)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'attention_scores': attention_scores,\n",
    "        'lime_scores': lime_scores,\n",
    "        'shap_scores': shap_scores\n",
    "    })\n",
    "\n",
    "# Apply to entire dataframe (iterates through each row)\n",
    "ucb_hate_speech_df[['attention_scores', 'lime_scores', 'shap_scores']] = ucb_hate_speech_df.apply(compute_all_explanations, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention and SHAP features match perfectly!\n"
     ]
    }
   ],
   "source": [
    "attention_scores_length = ucb_hate_speech_df[\"attention_scores\"].apply(len).tolist()\n",
    "shap_scores_length = ucb_hate_speech_df[\"shap_scores\"].apply(len).tolist()\n",
    "different_length = []\n",
    "\n",
    "for i in range(len(attention_scores_length)):\n",
    "    if attention_scores_length[i] != shap_scores_length[i]:\n",
    "        different_length.append(i)\n",
    "\n",
    "if len(different_length) > 0:\n",
    "    print(f\"There are different lengths for attentions and SHAP features in: {different_length}\")\n",
    "    \n",
    "    ucb_hate_speech_df = ucb_hate_speech_df.drop(ucb_hate_speech_df.index[different_length])\n",
    "    ucb_hate_speech_df = ucb_hate_speech_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Deleted {len(different_length)} rows\")\n",
    "    print(\"UC Berkerly Hate Speech Dataset Filtered:\", ucb_hate_speech_df.shape)\n",
    "else:\n",
    "    print(\"Attention and SHAP features match perfectly!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07448fc0518c40108e238f5bb5e9ba06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a96673577ae4ed6b1fc5fd813677ff6",
      "max": 201,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_424e0c8b7b464f18af0ff73a05bcbfd9",
      "value": 201
     }
    },
    "1b45d312a84549c2b0087c8c2a9d0fef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed39a1188b39478fb53f9629dd8b336f",
      "placeholder": "​",
      "style": "IPY_MODEL_8e166f1cc89d46688bd996bab2abd851",
      "value": " 201/201 [00:01&lt;00:00, 225.18it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]"
     }
    },
    "2e1e96ad741d410bb0d49b314cb32780": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b643c85b348a4e73be4e896ca4c52f68",
      "placeholder": "​",
      "style": "IPY_MODEL_99cabc6ec9c244868bc65a43d6f9ba29",
      "value": " 32/240 [00:50&lt;06:03,  1.75s/it]"
     }
    },
    "388d35a6987f47b9bdf8189b20f05ed1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4118c7ef88df4f2a8a47c3cf06ea05b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee4eaa72e69b4004bd77028c51adec81",
      "placeholder": "​",
      "style": "IPY_MODEL_418938143165450292dcd437f011bebf",
      "value": " 13%"
     }
    },
    "418938143165450292dcd437f011bebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "424e0c8b7b464f18af0ff73a05bcbfd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "433bc8baacd949269165c2671315c317": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b56f6bc2413b4cd38c85973e82090cee",
      "max": 240,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68fdc1af6223493bbabd352c23fffa14",
      "value": 32
     }
    },
    "4b66b10e32e6418ebd6609fddb7f89d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6fe071170224a0e95eb69886eac81cf",
       "IPY_MODEL_07448fc0518c40108e238f5bb5e9ba06",
       "IPY_MODEL_1b45d312a84549c2b0087c8c2a9d0fef"
      ],
      "layout": "IPY_MODEL_388d35a6987f47b9bdf8189b20f05ed1"
     }
    },
    "51460b200be74fa4bcbc2799f3d44ca4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68fdc1af6223493bbabd352c23fffa14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a96673577ae4ed6b1fc5fd813677ff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e166f1cc89d46688bd996bab2abd851": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99cabc6ec9c244868bc65a43d6f9ba29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b56f6bc2413b4cd38c85973e82090cee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b643c85b348a4e73be4e896ca4c52f68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c18f8ba8e0ce4db3a3e46239c7ece73b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4118c7ef88df4f2a8a47c3cf06ea05b0",
       "IPY_MODEL_433bc8baacd949269165c2671315c317",
       "IPY_MODEL_2e1e96ad741d410bb0d49b314cb32780"
      ],
      "layout": "IPY_MODEL_51460b200be74fa4bcbc2799f3d44ca4"
     }
    },
    "d92d41f6e1194b8f8eca122e089aab4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed39a1188b39478fb53f9629dd8b336f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee4eaa72e69b4004bd77028c51adec81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6fe071170224a0e95eb69886eac81cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc677ea51c894e37b920e61580dbafc9",
      "placeholder": "​",
      "style": "IPY_MODEL_d92d41f6e1194b8f8eca122e089aab4f",
      "value": "Loading weights: 100%"
     }
    },
    "fc677ea51c894e37b920e61580dbafc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
